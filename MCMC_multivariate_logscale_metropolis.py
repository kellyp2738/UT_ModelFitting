#!/usr/bin/python

# ----------------------------------------------------------------------------------------------
# -- MCMC_multivariate_logscale_metropolis.py --
# ----------------------------------------------------------------------------------------------

# -- What this script does:
#   -- Imports the 'VeggieDeath' SIRS model for model fitting
#   -- Runs an MCMC process on parameters and data supplied by the user as .csv files
#   -- Outputs the MCMC chain and several diagnostic files to the directory specified by the user
#   -- For more detailed usage notes: $ MCMC_multivariate_logscale_metropolis.py -h

# ----------------------------------------------------------------------------------------------
# -- Bugs to fix
# ----------------------------------------------------------------------------------------------

# -- MCMC chain saving
# --- Intermittent saving of the MCMC chain does not save the data as comma separated
# --- There are new lines where there shouldn't be and no commas where there should be...
# --- regex from cmd line should fix it (need to figure out the command though)
# --- NOT a problem if the model run completes and the final log is saved...

# ----------------------------------------------------------------------------------------------
# -- Import modules --
# ----------------------------------------------------------------------------------------------

from __future__ import division #because otherwise 1/2 = 0. Thanks, python!
import pdb
import copy
import random as rand
#rand.seed(0) # set the seed for the random number generator 
import numpy as np
#np.random.seed(0) #set the seed for the random number generator (different seed from above)
from scipy import integrate
from scipy import stats
import scipy
import math
#import pylab
#import matplotlib.mlab as mlab
#import matplotlib.pyplot as plt
from decimal import *
import decimal
import time
import csv
import os
import sys, getopt
from math import pi

import VeggieDeath_multivariate_logscale as VD
from scipy.special._ufuncs import logit

# ----------------------------------------------------------------------------------------------
# -- Parse command line arguments --
# -- see $ MCMC_lite.py -h for more details
# ----------------------------------------------------------------------------------------------

if __name__ == '__main__':
    
    import argparse
    
    parser = argparse.ArgumentParser(description = 'Fits compartmental deterministic tick-borne disease model variants to prevalence data via Bayesian MCMC')
    parser.add_argument('-prev', '--prevalence_file', help = 'CSV file containing prevalence data')
    parser.add_argument('-out', '--output_dir', help = 'Path for writing output data')
    parser.add_argument('-it', '--iterations', help = 'Number of iterations for MCMC')
    parser.add_argument('-p', '--plot', help = 'Plot SIR curves? Will not plot if iterations > 1')
    parser.add_argument('-pop', '--pop_size', help = 'Vector of animal population sizes')
    parser.add_argument('-racc', '--racc_density', help = 'Number of raccoons')
    parser.add_argument('-rand', '--random_chain_start', help = 'Choose random starting parameter for chain? True/False')
    
    opts = parser.parse_args()

# ----------------------------------------------------------------------------------------------
# -- Define main functions --
# ----------------------------------------------------------------------------------------------

# -- AdaptiveSteps() allows the hard-coded step sizes to be modulated to improve the acceptance ratio of the MCMC chain

def AdaptiveSteps(proposed, accepted, i_min, i_max):

    # -- initialize a vector to store the acceptance rates
    rates=[]
    
    # -- for every parameter to be estimated, calculate the acceptance rate and append that rate to the vector initialized above
    for col in range(len(proposed[0])): # for each parameter
        rate = sum(accepted[i_min:i_max, col])/sum(proposed[i_min:i_max, col])
        rates.append(rate)

    # -- return the rates as an object
    return rates
    
# -- BindLogs() takes the wealth of numpy arrays generated by the MCMC function and pastes them to gether so they can be saved as a single .csv file    

def BindLogs(accepted_log_names, iteration_log, model_out_log, prev_log, lik_log, prior_log, post_log, t_eq_log, accept_rate_log, proposed_log_names, prop_iteration_log, prop_model_out_log, prop_prev_log, prop_lik_log, prop_prior_log, prop_post_log, par_bounds_log, prop_lik_bounds_log, prop_t_eq_log): # all the logs ... removed arguments: is_updated_log, Paccept_reject_log, update_index_log, 
    
    # -- concatenate all the accepted ('used') logs for the main MCMC chain
    # -- this is the key and most important output
    used_log=np.concatenate((iteration_log, model_out_log, prev_log, lik_log, prior_log, post_log, t_eq_log, accept_rate_log), axis=1) #, is_updated_log, Paccept_reject_log), axis=1) # bind as rows
    used_save=np.array(used_log, dtype='|20S') # put in a saveable data type
    #print accepted_log_names
    #print 'lenghts', len(accepted_log_names), len(used_save[0])
    used_final=np.vstack((accepted_log_names, used_save))
    
    
    # -- concatenate all the proposed logs from the MCMC
    # -- this file is useful for diagnostic purposes but plays no main role in analysis
    proposed_log=np.concatenate((prop_iteration_log, prop_model_out_log, prop_prev_log, prop_lik_log, prop_prior_log, prop_post_log, par_bounds_log, prop_lik_bounds_log, prop_t_eq_log), axis = 1) #removed arugments: update_index_log, 
    proposed_save=np.array(proposed_log, dtype='|20S')
    proposed_final=np.vstack((proposed_log_names, proposed_save))
    
    # -- return the two array objects
    return used_final, proposed_final

# -- prior() calculates the prior probability of parameters
# -- this function assumes all priors are normally distributed

def prior(params, run_pars, est_index, i):
    
    # -- initialize a storage vector for the prior probabilities of parameters
    prior_dist=[]
    
    # -- look up the mean of the parameter prior dist (params[1][index]), look of the sd of the parameter prior dist (params[3][index]), and evaluate the log(pdf) at the relevant proposed parameter value, run_pars[index].
    for index in est_index:
        prior_dist.append(scipy.stats.norm(params[1][index], params[3][index]).logpdf(run_pars[index])) 
    
    prior_prob=Decimal(sum(prior_dist))
    
    # -- return the prior probability as an object
    return prior_prob

# -- calculate the log lik of the prevalence values sampled on the logit scale

def norm_lik_logit_scale(prev_distr, output_prevs):

    # -- remove nymph data from output_prevs to avoid confusion over indexes
    output_prevs = output_prevs[1:] # remove index 0
    #print 'output prevs', output_prevs

    # -- logit transform the model outputs
    psi=[]
    for theta in output_prevs:
        psi.append(math.log(theta/(1-theta)))
    #print 'psi values', psi
    #print 'out prev  ', output_prevs
    
    # -- storage vector for log likelihoods
    loglik=[]
    
    # -- calculate individual log likelihoods # EXCLUDING THE RACCOON DATA
    for i in range(0,3):
        #print 'animal index', i, 'mean', prev_distr[0][i], 'sd', prev_distr[1][i]
        lik=Decimal(scipy.stats.norm(prev_distr[0][i], prev_distr[1][i]).pdf(psi[i]))
        ll=Decimal(lik).ln()
        if math.isnan(ll):
            return 'fail 1'
        elif math.isinf(ll):
            return 'fail 2'
        else:
            loglik.append(ll)
    
        loglik.append(ll)
    
    # -- sum to final likelihood score
    
    #print 'individual neg log lik ', loglik
    final_ll=Decimal(sum(loglik))
    #print 'negative log likelihood ', final_ll
    
    return final_ll

# -- MCMC() is the brains of the operation

def MCMC(prev_data, dir_name, burnin, iterations, pop_sizes, racc_pop, plot = False):
    
    #pdb.set_trace()
    
    # -- MCMC_lite.py will only run the 'VeggieDeath' variant of the SIR model. Check that this is correctly specified
    # -- aquire parameters and assign functions as specified by the module VD
    
    # ----------------------------------------------------------------------------------------------
    # -- Randomly draw parameters and record their log(value)
    # ----------------------------------------------------------------------------------------------
    
    #params = [math.log(np.random.uniform()), math.log(np.random.uniform()), math.log(np.random.uniform()), math.log(np.random.uniform())] #draw four random pars to seed the chains
    # seed chain with parameters drawn from [0,1]
    params = [np.random.uniform(), np.random.uniform(), np.random.uniform(), np.random.uniform()]
    
    # ----------------------------------------------------------------------------------------------
    # -- Set SIR function to use
    # ----------------------------------------------------------------------------------------------
    
    SIR = VD.SIRprevalences # set the SIR function to use
    calculate_prev = VD.calculate_prev
    mkPlot = VD.mkPlot
    
    # ----------------------------------------------------------------------------------------------
    # -- Set the likelihood fxn (use this to sub other likelihood functions if needed)
    # ----------------------------------------------------------------------------------------------
    
    calc_lik = norm_lik_logit_scale
    
    # ----------------------------------------------------------------------------------------------
    # -- HOUSEKEEPING For runs with >1 iteration --
    # ----------------------------------------------------------------------------------------------
    
    # -- each run gets a unique name containing the timestamp and a randomly generated number to distinguish runs started at the same time
    # -- the random unique ID only works if the random number seed is not set
    run_start_time_stamp = time.strftime("%Y-%m-%d %H-%M") + " " + str(int(np.random.uniform(100,1000))) # Get a unique time stamp for the run, make file names to be used for all outputs, and save copies of parameters and prevalence priors used so runs can be reconstructed
    
    # -- print run-identifying information to screen
    print dir_name
    print 'Model: estimate preference and specified transmission in an on- and -off host tracking SIR model with block updating and log-scale parameter proposals'
    print run_start_time_stamp
    
    # -- set up the directory path for saving the data
    if not os.path.exists(dir_name):
        os.makedirs(dir_name)
        
    # -- establish the names to use for saving files
    accepted_run_name = dir_name + '/1 MCMC Accepted Iteration Log '+ run_start_time_stamp + '.csv' # file paths are relative to the current directory, and thus don't need a preceeding '/'
    proposed_run_name = dir_name + '/2 MCMC Proposed Iteration Log '+ run_start_time_stamp + '.csv' # file paths are relative to the current directory, and thus don't need a preceeding '/'
    params_name = dir_name + '/3 Parameters ' + run_start_time_stamp + '.csv'
    prev_data_name = dir_name + '/4 Prev Data ' + run_start_time_stamp + '.csv'
    racc_size_name = dir_name + '/7 Racc Population Size ' + run_start_time_stamp + '.csv'
    
    # -- resave some of the inputs to the MCMC so they can be preserved
    resave_params=np.array(params, dtype='|20S') # turn params into numpy array with 20 char string data type
    resave_prev_data=np.array(prev_data, dtype='|20S') # do the same for the priors
    np.savetxt(params_name, resave_params, delimiter=',', fmt='%20s') #specify format yet again, just to be safe
    np.savetxt(prev_data_name, resave_prev_data, delimiter=',', fmt='%20s')
    rp=open(racc_size_name, 'w')
    rp.write(str(racc_pop))
    rp.close()
    
    # ----------------------------------------------------------------------------------------------
    # -- SET UP THE DATA STRUCTURE FOR THE CHAINS
    # ----------------------------------------------------------------------------------------------
    
    # The basic framework for saving model and MCMC outputs is a series of numpy arrays that will be modified with each iteration and concatenated at the end of the run.
    # Each row represents an iteration, and each column represents a unique value (parameter, model output, prevalence, or statistic (e.g., likelihood) for that iteration
    # Arrays are made of blocks of data expected to be updated simultaneously. For example, the model parameters are one unified block of data. Even though only one par
    # will be changed at a time, updating an entire row at once is sensible. Alternatively, likelihoods, priors, and posteriors are calculated separately and dealt with in 
    # different sections of the code. Consequently these will have their own vectors. This relaxes the need to remember which column index belongs to which value.
    # All columns will be given name vectors that will be concatenated into a row of names and then to the data themselves. This is because naming rows in numpy arrays is
    # like staring into the eye of Sauron and no one wants that.
    
    # -- Make some arrays that will store the parameter chains --
    par_names = ['phiD', 'phiA', 'rhoTD', 'rhoDT'] # names that will be used for the main iteration log containing only accepted par sets
    prop_par_names = [] # empty container to store the proposal names, which are teh same as the par_names, only with '_prop' added to differentiate them
    for q in par_names:                     #
        prop_par_names.append(q+'_prop')    # adjust the names
    iteration_log = np.array([[0.0]*len(par_names)]*iterations) # an array to store the accepted proposals
    prop_iteration_log = np.array([[0.0]*len(par_names)]*iterations) # an array to store all proposals; should have EXACTLY the same shape as the other array
    
    # -- Put the first set of parameters in each chain
    iteration_log[0] = params # row 1 of params has the starting values
    prop_iteration_log[0] = params
    
    # -- Make additional arrays to store the accepted data
    model_names=['L-D', 'SN-D', 'SA-D', 'IN-D', 'IA-D', 'L-R', 'SN-R', 'SA-R', 'IN-R', 'IA-R', 'SD', 'ID', 'RD', "S(L) off", "S(L/N) off", "S(N/A) off", "S(A) off", "I(L/N) off", "I(N/A) off", "I(A) off"] #names for periodic time series saving
    model_out_log = np.array([[0.0]*len(model_names)]*iterations) # Make some arrays that will store the model outputs (number of individuals in a compartment. Right now the shape of this is hard coded based on the number of model compartments which is not expected to change
    prev_names=['nPrev','aPrev','dPrev','dABprev']
    prev_log = np.array([[0.0]*len(prev_names)]*iterations) # Make an array to store the model prevalences. Again, this is hard coded and not envisioned to be flexible. God save us if we need to start calculating different prevalences.
    lik_name=['log_likelihood']
    lik_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    prior_n=['prior']
    prior_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    posterior=['posterior']
    post_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    t_eq=['time_to_eq'] # the length of the model output time series so that we can track how long it takes the models to reach steady state
    t_eq_log = np.zeros(shape=(iterations, 1))
    accept_reject=['accept_reject']
    accept_rate_log = np.zeros(shape=(iterations, 1))
    
    # -- Make additional arrays to store the proposed data
    prop_model_names=['L-D', 'SN-D', 'SA-D', 'IN-D', 'IA-D', 'L-R', 'SN-R', 'SA-R', 'IN-R', 'IA-R', 'SD', 'ID', 'RD', "S(L) off", "S(L/N) off", "S(N/A) off", "S(A) off", "I(L/N) off", "I(N/A) off", "I(A) off"] #names for periodic time series saving
    prop_model_out_log = np.array([[0.0]*len(prop_model_names)]*iterations)
    prop_prev_names=['prop_nPrev','prop_aPrev','prop_dPrev','prop_dABprev']
    prop_prev_log = np.array([[0.0]*len(prop_prev_names)]*iterations)
    prop_lik_name=['prop_log_likelihood']
    prop_lik_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    prop_prior_n=['prop_prior']
    prop_prior_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    prop_posterior=['prop_posterior']
    prop_post_log = np.zeros(shape=(iterations, 1), dtype=np.dtype(decimal.Decimal))
    par_bounds=['prop_in_bounds']
    par_bounds_log=np.zeros(shape=(iterations, 1))
    prop_lik_in_bounds=['prop_lik_in_bounds']
    prop_lik_bounds_log = np.zeros(shape=(iterations, 1))
    prop_t_eq=['prop_time_to_eq'] # the length of the model output time series so that we can track how long it takes the models to reach steady state
    prop_t_eq_log = np.zeros(shape=(iterations, 1))
    
    # DEPRECATED: logging of update index for single component fitting
    #update_index=['update_index']
    #update_index_log = np.zeros(shape=(iterations, 1))
    # -- start a counter that tracks the total number of accepted proposals
    #accept_total=0 #start the counter
    
    # -- rename a variable out of laziness
    initial_pops = pop_sizes # give this guy a new name to match with what was already in the SIRPrevalences calls. laziness!
    
    # ----------------------------------------------------------------------------------------------
    # -- Step size adjustment logs (see old versions of code for single component proposal step size adjustment)
    # ----------------------------------------------------------------------------------------------
    
    # -- Specify the iterations at which to compute per-parameter acceptance rates and adjust step sizes
    interval=5000
    check_steps=range(0+interval,50000+interval,interval)
    
    # -- Build a data structure for storing the adaptive step size info
    steps_log=np.zeros(shape=((len(check_steps)+1), 1)) # with block updating, all parameters have the same acceptance rate
    steps_log[0]=0.03 #set the first step
    
    # -- Make a vector to store the acceptance rates used in step size calibration
    a_rates=np.zeros(shape=((len(check_steps)+1), 1))
    
    # -- Make a vector to store the iterations at which step sizes were checked
    it_log = np.zeros(shape=(len(check_steps)+1, 1))

    # -- Make a list of names for the step size log as a numpy array (keeping the data types consistent enables building the full log in the end)
    it_names = ['iteration']
    s_names = ['step_size'] # same for all parameters
    a_names = ['acceptance_rate']
    step_size_names=np.concatenate((it_names, s_names, a_names))
    
    # -- Start a counter for how many times step size has been checked
    cs = 0
    
    ## SINGLE COMPONENT STEP SIZE ADJUSTMENT CODE REMOVED
    
    # -- DEPRECATED
    # tracking the index of estimated params is necessary if prior probabilities are ever assigned to their values
    # therefore, I'm keeping this definition in the code in case it's useful in the future
    #est_index=['a', 'a', 'a'] #there are three params to estimate; instead of changing est_index to 3 everwhere, we just define it once here as a vector with length 3.
    
    # -- DEPRECATED
    # we don't need to keep track of which parameter is updated because the current model uses block updating
    #is_updated=[]
    #Paccept_reject=[]
    #is_updated_log=np.zeros(shape=(iterations, len(est_index))) # log 0 or 1 for which par was updated
    #Paccept_reject_log=np.zeros(shape=(iterations, len(est_index))) # a log for the accept/reject by parameter -- should be a 0 or 1
       
    # ----------------------------------------------------------------------------------------------
    # -- Paste all the names together for the storage of one big unified chain
    # ----------------------------------------------------------------------------------------------
    
    accepted_log_names=np.concatenate((par_names, model_names, prev_names, lik_name, prior_n, posterior, t_eq, accept_reject)) #, is_updated, Paccept_reject))
    proposed_log_names=np.concatenate((prop_par_names, prop_model_names, prop_prev_names, prop_lik_name, prop_prior_n, prop_posterior, par_bounds, prop_lik_in_bounds, prop_t_eq)) #removed arguments: update_index, 
    
    # ----------------------------------------------------------------------------------------------
    # -- THE MCMC
    # ----------------------------------------------------------------------------------------------
        
    #for i in range(iterations):
    fail_count=0
    for i in range(100000): # generate a pilot proposal distribution
    
        # ----------------------------------------------------------------------------------------------
        # -- The first iteration seeds the chain
        # ----------------------------------------------------------------------------------------------
    
        if i==0: # in the first iteration, run the model with the first set of parameters... this requires some non-obvious updates to both the accepted and proposed logs to kick everything off.  
            #run_pars=[math.exp(prop_iteration_log[0,0]), math.exp(prop_iteration_log[0,1]), math.exp(prop_iteration_log[0,2]), math.exp(prop_iteration_log[0,3])] # these are the starting values copied over from the params input file
            # no need to exponentiate; current version draws first parameter set from the appropriate interval [0,1]
            run_pars=[prop_iteration_log[0,0], prop_iteration_log[0,1], prop_iteration_log[0,2], prop_iteration_log[0,3]] # these are the starting values copied over from the params input file
            print 'starting parameters', run_pars
            #return
            #run_pars=prop_iteration_log[0]
            r_all=SIR(run_pars, initial_pops, 1000, 0, 1, racc_pop)
            r=r_all[0]
            time_to_eq=r_all[1]
            if r == 'failed to reach equilibrium':
                model_out_log[0]=[-999]*len(model_names) #fill pop sizes w/ridiculous num to signal model failure
                print r, 'Final Iteration 0'
                return # don't bother to save anything
            else: # if we made it to equilibrium
                model_output=r[-1:][0] # this is a 3D array when it could really be a 2D array... the element 0 of the last row is what actually contains the data vector
                prop_model_out_log[0]=model_output
                model_out_log[0]=model_output
                prop_t_eq_log[0]=len(r) 
                t_eq_log[0]=len(r) #gets the number of rows
                if np.amin(model_output) >= 0: #if all the compartments have animals in them
                    prevs = calculate_prev(model_output) # calculate the prevalences
                    #print prevs
                    prev_log[0] = prevs # put those in the log too
                    prop_prev_log[0] = prevs
                    ll=calc_lik(prev_data, prevs) # calculate the log-likelihood of the output, and the prior prob of the starting parameters (if the parameters have informative distributions)                  
                    prop_lik_log[0] = ll
                    lik_log[0] = ll
                    # -- if the likelihood is negative, we can't take its log and the MCMC cannot proceed
                    if ll == 'fail 1':
                        error_msg = 'Failed due to negative likelihood value. See output prior to failure in file ' + accepted_run_name
                        print error_msg, 'Final Iteration ', i
                        return 
                    elif ll== 'fail 2':
                        print 'Infinite log likelihood, iteration', i
                        print dir_name
                        return
                    #else:
                    #    if param_priors == True:
                    #        p=prior(params, run_pars, est_index, i)
                    #        prior_log[i] = p
                    #        prop_prior_log[i] = p
                    #        post = ll + p
                    #    else:
                    #        post = ll
                    #    prop_post_log[i]=post
                    #    post_log[i]=post
                    else:
                        post = ll
                        prop_post_log[i]=post
                        post_log[i]=post
                else: # only if the model fails on the first iteration should we return this error
                    error_msg = 'First iteration failed -- negative animals.'
                    print error_msg, min(model_output)
                    return # kill it; there's a bug.
                    
        # ----------------------------------------------------------------------------------------------
        # -- Build the pilot distribution
        # ----------------------------------------------------------------------------------------------
        
        else: # do pilot runs to generate a proposal distribution
            #print i
            
            # these parameters are on the log scale
            #log_theta_star=[np.random.normal(iteration_log[i-1, 0], 0.1), np.random.normal(iteration_log[i-1, 1], 0.1), np.random.normal(iteration_log[i-1, 2], 0.1)]
            # the exponentiated params to pass to the SIR fxn
            #theta_star=[math.exp(log_theta_star[0]), math.exp(log_theta_star[1]), math.exp(log_theta_star[2])]
            #prop_iteration_log[i] = log_theta_star
            
            # sample the transformed params to get proposals from a symetric distribution
            
            #should you want to use a fixed step size, use this line:
            #theta_star=[np.random.normal(math.exp(iteration_log[i-1, 0]), 0.05), np.random.normal(math.exp(iteration_log[i-1, 1]), 0.05), np.random.normal(math.exp(iteration_log[i-1, 2]), 0.05)]
            
            # ----------------------------------------------------------------------------------------------
            # -- Check step sizes
            # ----------------------------------------------------------------------------------------------
    
            if (i in check_steps) == True: # once we've done a few iterations, but while we're still in the burnin, we can adjust the step size to ensure proper mixing
                #if i == interval:
                #	old_step = 0.5
                #else:
                #	old_step = steps_log[cs-1]
                it_log[cs+1]=i #log which iteration we're adjusting at
                rate=sum(accept_rate_log[i-interval:i])/interval #number of times parameter proposal was accepted
                a_rates[cs]=rate #should be at index [cs] b/c this is the acceptance rate for the previous iteration range
                #print rate
                if rate > 0.3:
                	steps_log[cs+1] = steps_log[cs] + 0.0005 #increase step size so more proposals get rejected
                elif rate < 0.1: #decrease step size so fewer proposals get rejected
                	if steps_log[cs] <= 0.0005:
                		new_step = 0.00001
                		steps_log[cs+1] = new_step
                	else:
                		new_step = steps_log[cs] - 0.0005
                		steps_log[cs+1] = new_step
                else:
                    steps_log[cs+1] = steps_log[cs]
                #print('iteration log value', it_log[cs+1], 'check step', cs, 'acceptance rate', rate, 'new step', steps_log[cs+1])
                
            	cs+=1          
            
            # ----------------------------------------------------------------------------------------------
            # -- Save the step size adaptation log
            # ----------------------------------------------------------------------------------------------
    
            if i==(max(check_steps)+1): #once we're just out of this step adaptation   
                save_steps_data = np.concatenate((it_log, steps_log, a_rates), axis = 1) # bind columns
                step_size_log = np.vstack((step_size_names, save_steps_data))
                save_name = dir_name + '/5 adaptive steps ' + run_start_time_stamp + '.csv' # save our mini step-size log
                save_steps = np.array(step_size_log, dtype = '|20S')
                np.savetxt(save_name, save_steps, delimiter=',', fmt='%20s')
                
            # ----------------------------------------------------------------------------------------------
            # -- Draw new parameters
            # ----------------------------------------------------------------------------------------------
    
            # -- Dynamic step size adjustment during the pilot runs
            #theta_star=[np.random.normal(math.exp(iteration_log[i-1, 0]), steps_log[cs]), np.random.normal(math.exp(iteration_log[i-1, 1]), steps_log[cs]), np.random.normal(math.exp(iteration_log[i-1, 2]), steps_log[cs]), np.random.normal(math.exp(iteration_log[i-1, 3]), steps_log[cs])]
            
            def myLogit(x):
                return np.math.log((x/(1-x)))
            
            def myiLogit(x):
                return 1/(1+np.math.exp(-x))
            
            # -- use the logit transform of the param to define the sampling distribution
            theta=[np.random.normal(myLogit(iteration_log[i-1, 0]), steps_log[cs]), np.random.normal(myLogit(iteration_log[i-1, 1]), steps_log[cs]), np.random.normal(myLogit(iteration_log[i-1, 2]), steps_log[cs]), np.random.normal(myLogit(iteration_log[i-1, 3]), steps_log[cs])]
            
            theta_star = myiLogit(theta)
            
            #print 'new params', theta_star
            #print 'new log params', log_theta_star
            
            in_bounds_prefD = (0 < theta_star[0] < 1)
            in_bounds_prefA = (0 < theta_star[1] < 1)
            in_bounds_rho = (0 < theta_star[2] < 1)
            in_bounds_rho2 = (0 < theta_star[3] < 1)
            
            # -- Are the parameters in bounds?
            if in_bounds_prefD and in_bounds_prefA and in_bounds_rho and in_bounds_rho2:
            
                #prop_iteration_log[i] = [math.log(theta_star[0]), math.log(theta_star[1]), math.log(theta_star[2]), math.log(theta_star[3])]
                prop_iteration_log[i] = theta_star # record the inverse logit values, which will range betweeon [0,1]
            
                # -- Run the SIRS model with the new parameters
                par_bounds_log[i] = 1 # indicate that the par was in bounds           
                #if i > 2300:
                #   print 'phi', run_pars[0]
                #print 'new phi value', new
                r_all=SIR(theta_star, initial_pops, 1000, 0, 1, racc_pop) #log-scale params get exponentiated in the SIR module
                r=r_all[0]
                #print 'population sizes', r
                time_to_eq=r_all[1]
                
                # -- Did the model reach equilibrium?
                if len(r) > 1: # != 'failed to reach equilibrium': # if it didn't fail
                    
                    # -- Extract the last time step, which reflects equilibrium conditions
                    model_output=r[-1:][0] # take the last entry from the model time series
                    prop_model_out_log[i]=model_output #put the model outputs into the iteration log
                    prop_t_eq_log[i]=time_to_eq
                    #prop_t_eq_log[i]=len(r) #gets the number of rows
                    
                    # Do the compartment sizes make sense? (e.g., no negative animals?)
                    if min(model_output) >= 0: #if all the compartments have animals in them (if there are neg animals, then we move on without updating anything)
                        
                        # -- Calculate prevalences and log them
                        prevs=calculate_prev(model_output)
                        #print 'prevalence', prevs
                        prop_prev_log[i]=prevs
                        
                        # -- Calculate the log likelihood
                        ll=calc_lik(prev_data, prevs)
                        #print 'll', ll
                        #if i > 2300:
                        #   print 'likelihood', ll
                        
                        # -- Could a log likelihood be calculated?
                        possible_failures = ['fail 1', 'fail 2']
                        test_var = ll in possible_failures
                        if test_var == False: # if the value of the likelihood is not one of the possible fail codes
                        
                            # -- record the log likelihood in the data array
                            prop_lik_log[i]=ll # we can't automatically add this to the array b/c if it's 'fail' that's a string and it can't go into a numpy array of dtype float
                            prop_lik_bounds_log[i] = 1 #in bounds
                            
                            # -- calculate prior probabilities where appropriate
                            #if param_priors == True:
                            #    p=prior(params, run_pars, est_index, i)
                            #    prior_log[i] = p
                            #    prop_prior_log[i] = p
                            #    post = ll + p
                            #else:
                            #    post = ll
                            post = ll
                            prop_post_log[i]=post
                            old_post = post_log[i-1][0] # retrieve the most recent ACCEPTED posterior
                            #print 'old ll', old_post
                            
                            # ----------------------------------------------
                            # -- calculate the probability of acceptance
                            # ----------------------------------------------
                            
                            # -- note: if sampling params from non-symmetrical distribution, incl. the ratio of values in the accept/reject calculation
                            
                            # -- deprecated: acceptance ratio for Metropolis algorithm
                            # -- for testing the code, we're using the incorrect acceptance criterion (need guidance from JS on how to do it properly)
                            prob = Decimal(post-old_post).exp()
                            #print 'accept probability', prob
                            
                            # -- current: acceptance ratio for Metropolis-Hastings algorithm where transmission is drawn from an asymmetrical (lognormal) distribution
                            #r=(post-old_post)+Decimal(Jt1-Jt2)+Decimal(Jt12-Jt22) #Roz version
                            #r=(post-old_post)+Decimal(Jt2-Jt1)
                            #r=Decimal(math.exp(post)/math.exp(old_post))*Decimal(Jt2/Jt1)
                            #prob=Decimal(r).exp()
                            
                            # -- If we accept the proposal, record the new parameters in the chain
                            if np.random.uniform(0,1) < prob: # if we accept this new parameter
                                #print 'Accepted'
                                iteration_log[i] = prop_iteration_log[i] # fill in this par for all subsequent rows
                                model_out_log[i] = prop_model_out_log[i]
                                prev_log[i] = prop_prev_log[i]
                                lik_log[i] = prop_lik_log[i]
                                prior_log[i] = prop_prior_log[i]
                                post_log[i] = prop_post_log[i]
                                t_eq_log[i] = prop_t_eq_log[i]
                                accept_rate_log[i] = 1
                                #Paccept_reject_log[i][0] = 1
                                #Paccept_reject_log[i][1] = 1
                            
                            # -- If we don't accept the proposal, copy the most recently accepted parameter set into the row for this iteration
                            else: 
                                #print 'NOT ACCEPTED'
                                iteration_log[i] = iteration_log[i-1] # fill in this par for all subsequent rows
                                model_out_log[i] = model_out_log[i-1]
                                prev_log[i] = prev_log[i-1]
                                lik_log[i] = lik_log[i-1]
                                prior_log[i] = prior_log[i-1]
                                post_log[i] = post_log[i-1]
                                t_eq_log[i] = t_eq_log[i-1]   
                                #step_size_log[i] = step_size_log[i-1]
                                
                        # -- If we couldn't compute a log likelihood, note the error and return
                        else:
                            fail_count+=1
                            #print('Could not compute log likelihood. Cumulative number of failures:', fail_count)
                            iteration_log[i] = iteration_log[i-1] # fill in this par for all subsequent rows
                            model_out_log[i] = model_out_log[i-1]
                            prev_log[i] = prev_log[i-1]
                            lik_log[i] = lik_log[i-1]
                            prior_log[i] = prior_log[i-1]
                            post_log[i] = post_log[i-1]
                            t_eq_log[i] = t_eq_log[i-1]   
                            #step_size_log[i] = step_size_log[i-1]
                            
                    # -- If we have negative animals, discard the proposal and move on to the next iteration
                    else:  
                        iteration_log[i] = iteration_log[i-1]
                        model_out_log[i] = model_out_log[i-1]
                        prev_log[i] = prev_log[i-1]
                        lik_log[i] = lik_log[i-1]
                        prior_log[i] = prior_log[i-1]
                        post_log[i] = post_log[i-1]
                        t_eq_log[i] = t_eq_log[i-1]   
                        #step_size_log[i] = step_size_log[i-1]
                        
                # -- If the model didn't reach equilibruim, discard the proposal and move on to the next iteration
                else: 
                    prop_model_out_log[i]=[-999]*len(prop_model_names) #fill pop sizes w/ridiculous num to signal model failure
                    prop_t_eq_log[i]=-999 # no equilibrium; remainder of prop_logs remain at zero
                    iteration_log[i] = iteration_log[i-1]
                    model_out_log[i] = model_out_log[i-1]
                    prev_log[i] = prev_log[i-1]
                    lik_log[i] = lik_log[i-1]
                    prior_log[i] = prior_log[i-1]
                    post_log[i] = post_log[i-1]
                    t_eq_log[i] = t_eq_log[i-1]   
                    #step_size_log[i] = step_size_log[i-1]
                    
            # -- if a proposal is out of bounds, discard the proposal and move on to the next iteration
            else: 
                #accept_rate_log[i] = accept_total/i
                par_bounds_log[i] = 0
                iteration_log[i] = iteration_log[i-1]
                model_out_log[i] = model_out_log[i-1]
                prev_log[i] = prev_log[i-1]
                lik_log[i] = lik_log[i-1]
                prior_log[i] = prior_log[i-1]
                post_log[i] = post_log[i-1]
                t_eq_log[i] = t_eq_log[i-1]   
                #step_size_log[i] = step_size_log[i-1]
            
    for i in range(100000,iterations):   
    
        # ----------------------------------------------------------------------------------------------
        # -- Subsequent iterations build the chain
        # ----------------------------------------------------------------------------------------------
    
        # ----------------------------------------------------------------------------------------------
        # -- Incrementally save the data
        # ----------------------------------------------------------------------------------------------

        # -- save the whole log every 10,000 iterations
        if i%1000 == 0:
        
            # -- print the output so the user can keep track of progress
            print i
            
            # -- paste all the various numpy arrays together. they will have the same dimensions
            data = BindLogs(accepted_log_names, iteration_log, model_out_log, prev_log, lik_log, prior_log, post_log, t_eq_log, accept_rate_log, proposed_log_names, prop_iteration_log, prop_model_out_log, prop_prev_log, prop_lik_log, prop_prior_log, prop_post_log, par_bounds_log, prop_lik_bounds_log, prop_t_eq_log) # all the logs; removed arguments: is_updated_log, Paccept_reject_log, update_index_log, 
            accepted = data[0]
            
            # -- open a file (old logs will be overwritten) to save the data
            with open(accepted_run_name, 'a') as f:
                f.write(" ".join(map(str, accepted)))
            #np.savetxt(accepted_run_name, accepted, delimiter=',', fmt='%20s')
            
            proposed = data[1]
            with open(proposed_run_name, 'a') as g:
                g.write(" ".join(map(str, accepted)))               
            #np.savetxt(proposed_run_name, proposed, delimiter=',', fmt='%20s')

	    # ----------------------------------------------------------------------------------------------
        # -- Propose new parameters through a random walk
        # ----------------------------------------------------------------------------------------------

	    # get the covariance matrix for the proposal distribution
	    # proposal distribution is defined by ALL iterations after adaptive step size checking stops
    	for_cov=iteration_log[50000+interval:i,]
    	inflate=1.5
    	proposal_covariance=inflate*np.cov(for_cov, rowvar=0)
    	proposal_mean=iteration_log[i-1] # proposal distr. is centered on the last accepted value
        
        # sample on the log-scale from a multivariate normal distribution
        log_theta_star = np.random.multivariate_normal(proposal_mean, proposal_covariance) # return new par vector of same length as means vector
        theta_star = [math.exp(log_theta_star[0]), math.exp(log_theta_star[1]), math.exp(log_theta_star[2]), math.exp(log_theta_star[3])]
        prop_iteration_log[i]=log_theta_star # put it in the proposal log
        
        # ---------------------------------------------------------------------------------------------
        # -- Proposal revision: xi = xi-1 + N(0, alpha*cov)
        # ---------------------------------------------------------------------------------------------
        
        # get the covariance matrix for the proposal distribution
        # proposal distribution is defined by ALL iterations after adaptive step size checking stops
        #for_cov=iteration_log[50000+interval:i,]
        #inflate=1.5
        #proposal_covariance=inflate*np.cov(for_cov, rowvar=0)
        #last_proposal=iteration_log[i-1] # proposal distr. is centered on the last accepted value
        
        # sample on the log-scale from a multivariate normal distribution
        inflate = 1.5
        scaling_factor = np.random.multivariate_normal([0,0,0,0], 1.5*proposal_covariance) # the scaling factor is centered on zero and comes from the cov. matrix
        theta_star = [iteration_log[0]+scaling_factor[0], iteration_log[1]+scaling_factor[1], iteration_log[2]+scaling_factor[2], iteration_log[3]+scaling_factor[3]] # add the scaling factor to each previously accepted param
        prop_iteration_log[i]=log_theta_star # put it in the proposal log
        
        in_bounds_prefD = (0 < theta_star[0] < 1)
        in_bounds_prefA = (0 < theta_star[1] < 1)
        in_bounds_rho = (0 < theta_star[2] < 1)
        in_bounds_rho2 = (0 < theta_star[3] < 1)
        
        
        # -- Are the parameters in bounds?
        if in_bounds_prefD and in_bounds_prefA and in_bounds_rho and in_bounds_rho2:
        
            # -- Run the SIRS model with the new parameter
            par_bounds_log[i] = 1 # indicate that the par was in bounds           
            #if i > 2300:
            #   print 'phi', run_pars[0]
            #print 'new phi value', new
            r_all=SIR(theta_star, initial_pops, 1000, 0, 1, racc_pop) #log-scale params get exponentiated in the SIR module
            r=r_all[0]
            #print 'population sizes', r
            time_to_eq=r_all[1]
            
            # -- Did the model reach equilibrium?
            if len(r) > 1: # != 'failed to reach equilibrium': # if it didn't fail
                
                # -- Extract the last time step, which reflects equilibrium conditions
                model_output=r[-1:][0] # take the last entry from the model time series
                prop_model_out_log[i]=model_output #put the model outputs into the iteration log
                prop_t_eq_log[i]=time_to_eq
                #prop_t_eq_log[i]=len(r) #gets the number of rows
                
                # Do the compartment sizes make sense? (e.g., no negative animals?)
                if min(model_output) >= 0: #if all the compartments have animals in them (if there are neg animals, then we move on without updating anything)
                    
                    # -- Calculate prevalences and log them
                    prevs=calculate_prev(model_output)
                    #print 'prevalence', prevs
                    prop_prev_log[i]=prevs
                    
                    # -- Calculate the log likelihood
                    ll=calc_lik(prev_data, prevs)
                    #if i > 2300:
                    #   print 'likelihood', ll
                    
                    # -- Could a log likelihood be calculated?
                    possible_failures = ['fail 1', 'fail 2']
                    test_var = ll in possible_failures
                    if test_var == False: # if the value of the likelihood is not one of the possible fail codes
                    
                        # -- record the log likelihood in the data array
                        prop_lik_log[i]=ll # we can't automatically add this to the array b/c if it's 'fail' that's a string and it can't go into a numpy array of dtype float
                        prop_lik_bounds_log[i] = 1 #in bounds
                        
                        # -- calculate prior probabilities where appropriate
                        #if param_priors == True:
                        #    p=prior(params, run_pars, est_index, i)
                        #    prior_log[i] = p
                        #    prop_prior_log[i] = p
                        #    post = ll + p
                        #else:
                        #    post = ll
                        #prop_post_log[i]=post
                        #old_post = post_log[i-1][0] # retrieve the most recent ACCEPTED posterior
                        
                        post = ll
                        prop_post_log[i]=post
                        old_post = post_log[i-1][0] # retrieve the most recent ACCEPTED posterior
                        
                        # ----------------------------------------------
                        # -- calculate the probability of acceptance
                        # ----------------------------------------------
                        
                        # -- note: if sampling params from non-symmetrical distribution, incl. the ratio of values in the accept/reject calculation
                        
                        # -- acceptance ratio for Metropolis algorithm
                        prob = Decimal(post-old_post).exp()
                        
                        # -- If we accept the proposal, record the new parameters in the chain
                        if np.random.uniform(0,1) < prob: # if we accept this new parameter
                            #print 'Accepted'
                            iteration_log[i] = prop_iteration_log[i] # fill in this par for all subsequent rows
                            model_out_log[i] = prop_model_out_log[i]
                            prev_log[i] = prop_prev_log[i]
                            lik_log[i] = prop_lik_log[i]
                            prior_log[i] = prop_prior_log[i]
                            post_log[i] = prop_post_log[i]
                            t_eq_log[i] = prop_t_eq_log[i]
                            accept_rate_log[i] = 1
                            #Paccept_reject_log[i][0] = 1
                            #Paccept_reject_log[i][1] = 1
                        
                        # -- If we don't accept the proposal, copy the most recently accepted parameter set into the row for this iteration
                        else: 
                            iteration_log[i] = iteration_log[i-1] # fill in this par for all subsequent rows
                            model_out_log[i] = model_out_log[i-1]
                            prev_log[i] = prev_log[i-1]
                            lik_log[i] = lik_log[i-1]
                            prior_log[i] = prior_log[i-1]
                            post_log[i] = post_log[i-1]
                            t_eq_log[i] = t_eq_log[i-1]   
                            #step_size_log[i] = step_size_log[i-1]
                            
                    # -- If we couldn't compute a log likelihood, note the error and return
                    else:
                        fail_count+=1
                        #print('Could not compute log likelihood. Cumulative number of failures:', fail_count)
                        iteration_log[i] = iteration_log[i-1] # fill in this par for all subsequent rows
                        model_out_log[i] = model_out_log[i-1]
                        prev_log[i] = prev_log[i-1]
                        lik_log[i] = lik_log[i-1]
                        prior_log[i] = prior_log[i-1]
                        post_log[i] = post_log[i-1]
                        t_eq_log[i] = t_eq_log[i-1]   
                        #step_size_log[i] = step_size_log[i-1]
                        
                # -- If we have negative animals, discard the proposal and move on to the next iteration
                else:  
                    iteration_log[i] = iteration_log[i-1]
                    model_out_log[i] = model_out_log[i-1]
                    prev_log[i] = prev_log[i-1]
                    lik_log[i] = lik_log[i-1]
                    prior_log[i] = prior_log[i-1]
                    post_log[i] = post_log[i-1]
                    t_eq_log[i] = t_eq_log[i-1]   
                    #step_size_log[i] = step_size_log[i-1]
                    
            # -- If the model didn't reach equilibrium, discard the proposal and move on to the next iteration
            else: 
                prop_model_out_log[i]=[-999]*len(prop_model_names) #fill pop sizes w/ridiculous num to signal model failure
                prop_t_eq_log[i]=-999 # no equilibrium; remainder of prop_logs remain at zero
                iteration_log[i] = iteration_log[i-1]
                model_out_log[i] = model_out_log[i-1]
                prev_log[i] = prev_log[i-1]
                lik_log[i] = lik_log[i-1]
                prior_log[i] = prior_log[i-1]
                post_log[i] = post_log[i-1]
                t_eq_log[i] = t_eq_log[i-1]   
                #step_size_log[i] = step_size_log[i-1]
                
        # -- if a proposal is out of bounds, discard the proposal and move on to the next iteration
        else: 
            #accept_rate_log[i] = accept_total/i
            par_bounds_log[i] = 0
            iteration_log[i] = iteration_log[i-1]
            model_out_log[i] = model_out_log[i-1]
            prev_log[i] = prev_log[i-1]
            lik_log[i] = lik_log[i-1]
            prior_log[i] = prior_log[i-1]
            post_log[i] = post_log[i-1]
            t_eq_log[i] = t_eq_log[i-1]   
            #step_size_log[i] = step_size_log[i-1]
    
    # ----------------------------------------------------------------------------------------------
    # -- Save the final results
    # ----------------------------------------------------------------------------------------------
    
    data = BindLogs(accepted_log_names, iteration_log, model_out_log, prev_log, lik_log, prior_log, post_log, t_eq_log, accept_rate_log, proposed_log_names, prop_iteration_log, prop_model_out_log, prop_prev_log, prop_lik_log, prop_prior_log, prop_post_log, par_bounds_log, prop_lik_bounds_log, prop_t_eq_log) # all the logs; removed arguments: is_updated_log, Paccept_reject_log, update_index_log, 
    accepted = data[0]
    np.savetxt(accepted_run_name, accepted, delimiter=',', fmt='%20s')
    proposed = data[1]
    np.savetxt(proposed_run_name, proposed, delimiter=',', fmt='%20s')
    print 'Cumulative number of failures:', fail_count
    print 'Run completed.'
    return

# -- the lognormal pdf (Gelman et al. Bayesian Data Analysis 2nd Ed.)
def ln_norm_pdf(x, mu, sigma):
    #print(x, mu, sigma)
    part1=1/(x*math.sqrt(2*pi)*sigma)
    part2=-1*((math.log(x)-mu)**2)/(2*(sigma**2))
    all_together=Decimal(part1*math.exp(part2))
    #print('jump prob', all_together)
    return all_together

# -- get the prevalence distributions expressed in logit scale
def get_norm_logit_prevs(filename):
    prevs = np.genfromtxt(filename, delimiter=',')
    return prevs

# -- get_Binom_prev_priors() gathers the empirical prevalence data
def get_Binom_prev_priors(filename):
    priors=[]
    #filename=raw_input("Please enter the name of the prevalence data file. Do not use quotes. Ensure that the file to load is in the current directory and follows the format expected by this program! ")
    with open(filename, 'rU') as csvfile:
        prior_object=csv.reader(csvfile, delimiter=",")
        for row in prior_object:
            priors.append(row)
    for j in range(len(priors)):
        priors[j][1]=float(priors[j][1])
        priors[j][2]=float(priors[j][2])
        n_choose_k=Decimal((math.factorial(priors[j][2])/(math.factorial(priors[j][1])*math.factorial(priors[j][2]-priors[j][1]))))
        priors[j].append(n_choose_k) 
        priors[j].append(0) #make an empty spot to store prev
    return priors

# ----------------------------------------------------------------------------------------------
# -- Argument parsing and code execution
# ----------------------------------------------------------------------------------------------
  
# ----------------------------------------------------------------------------------------------
# -- Comment this whole bit out if importing modules from this script
# ----------------------------------------------------------------------------------------------

plot = opts.plot
#par_file = opts.param_file
#trans_type = opts.trans_type
iterations = int(opts.iterations)
pop_sizes_str=opts.pop_size
pop_sizes_split=pop_sizes_str.split(',')
pop_sizes=[]
racc_pop = int(opts.racc_density)
rand_chain_start = opts.random_chain_start

for item in pop_sizes_split:
    pop_sizes.append(int(item))
prev_file = opts.prevalence_file
prev_data = get_norm_logit_prevs(prev_file)
dir_name = opts.output_dir
iterations = int(opts.iterations)
    
# start the MCMC (burnin parameter is now hard-coded in the MCMC function, so its value here is set to 0)
MCMC(prev_data, dir_name, 0, iterations, pop_sizes, racc_pop, plot=False)



